# DQN GridWorld Demo

This tutorial demonstrates how to train a Deep Q-Network (DQN) agent on a simple GridWorld environment using Fehu.  
We will walk through agent creation, training, evaluation, and visualize the learning progress step-by-step.

**Goals:**
- Understand the basics of DQN and GridWorld
- See how to set up and train a DQN agent
- Visualize episode rewards and agent behavior

---

## Prerequisites

- Make sure you have Fehu, Kaun, Rune, Nx, and Hugin installed in your OCaml environment.
- To use plotting, install Hugin: `opam install hugin`
- This demo uses MDX, which allows executable OCaml code blocks in markdown documents.
- **Note:** Plot images are generated by running the plotting code in a standalone OCaml script or utop, not inside MDX.
- If you encounter errors, check your opam switch and library installations, and consult the documentation or maintainer.

---

## What is GridWorld?

GridWorld is a simple environment where an agent moves in a grid to reach a goal.  
It is commonly used for teaching reinforcement learning concepts because it is easy to visualize and understand.

---

## What is DQN?

Deep Q-Network (DQN) is a reinforcement learning algorithm that uses a neural network to approximate Q-values for each action in a given state.  
It enables agents to learn optimal policies in environments with discrete actions.

---

## 1. Create a Simple GridWorld Environment

```ocaml
open Fehu

(* Define the observation space as a 2D grid from (0,0) to (4,4) *)
let obs_space = Space.Box.create ~low:[| 0.0; 0.0 |] ~high:[| 4.0; 4.0 |] in

(* Define the action space: 4 possible moves (up, down, left, right) *)
let act_space = Space.Discrete.create 4 in

let env_rng = Rune.Rng.key 123 in
let step_count = ref 0 in

(* Create the environment with reset and step functions *)
let env =
  Env.create ~rng:env_rng ~observation_space:obs_space ~action_space:act_space
    ~reset:(fun _env ?options:_ () ->
      step_count := 0;
      let obs = Rune.create Rune.float32 [| 2 |] [| 0.0; 0.0 |] in
      (obs, Info.empty))
    ~step:(fun _env action ->
      step_count := !step_count + 1;
      let terminated = !step_count >= 10 in
      let obs =
        Rune.create Rune.float32 [| 2 |] [| float_of_int (!step_count mod 5); float_of_int (!step_count / 5) |]
      in
      Env.transition ~observation:obs ~reward:1.0 ~terminated ())
    ()
```

---

## 2. Create the DQN Agent

```ocaml
module Dqn = Fehu_algorithms.Dqn

let rng = Rune.Rng.key 42 in

(* Build a simple neural network for Q-value approximation *)
let q_net =
  Kaun.Layer.sequential
    [
      Kaun.Layer.linear ~in_features:2 ~out_features:8 ();
      Kaun.Layer.relu ();
      Kaun.Layer.linear ~in_features:8 ~out_features:4 ();
    ]
in

(* Configure the agent: batch size, buffer capacity, etc. *)
let config = Dqn.{ default_config with batch_size = 4; buffer_capacity = 50 } in
let agent = Dqn.create ~q_network:q_net ~n_actions:4 ~rng config
```

---

## 3. Train the DQN Agent and Collect Episode Rewards

```ocaml
let episode_rewards = ref [] in

(* Train the agent for 100 timesteps and collect rewards *)
let _agent =
  Dqn.learn agent ~env ~total_timesteps:100
    ~callback:(fun ~episode ~metrics ->
      episode_rewards := metrics.episode_return :: !episode_rewards;
      Printf.printf "Episode %d: Reward = %.2f\n%!" episode metrics.episode_return;
      true)
    ()
in

List.rev !episode_rewards  (* Show rewards per episode *)
```

---

## 4. Visualize Episode Rewards

Below we print the total reward per episode to see how the agent improves over time.

```ocaml
(* Print episode rewards for each episode *)
List.iteri (fun i r -> Printf.printf "Episode %d: Reward = %.2f\n%!" i r) (List.rev !episode_rewards)
```

---

### **Plotting with Hugin (outside MDX)**

> **Note:**  
> The following code should be run in a standalone OCaml script or utop, not inside MDX.  
> This will generate the plot image for display in your markdown notebook.

```ocaml
open Nx
open Hugin

let () =
  let rewards =
    Nx.create Nx.float32 [| List.length !episode_rewards |]
      (Array.of_list (List.rev !episode_rewards))
  in
  let fig = Hugin.plot_y rewards in
  Hugin.savefig "episode_rewards.png" fig
  Printf.printf "Saved plot to episode_rewards.png\n%!"
```

After running the code above, place `episode_rewards.png` in the same directory as this notebook.

---

### **Display the plot in markdown**

```markdown
![Episode Rewards](episode_rewards.png)
```

---

## Troubleshooting

- If you encounter errors running this notebook as an MDX document, check that all dependencies are installed and your OCaml version matches the project requirements.
- Common errors include missing libraries or version mismatches.  
  See the documentation or ask the maintainer for help if needed.
- If the plot image does not appear, make sure you generated it with the standalone script and placed it in the notebook directory.

---

## 5. Summary

- We trained a DQN agent on GridWorld and tracked its learning progress.
- You can experiment with different network architectures, hyperparameters, or environments.
- Try visualizing Q-values or agent trajectories for deeper insight!

---